{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP pre model activities - Distillation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP) \n",
    "Is the process of extracting information from text data. \n",
    "### Distillation \n",
    "Distill the unstructured or structured data source into a \n",
    "Condensed extracted representation of the unstructured data with weightages of importance and relevance.\n",
    "\n",
    "## 1. Stemming\n",
    "Stemming is the process of reducing the words(generally modified or derived) to their word stem or root form. The objective of stemming is to reduce related words to the same stem even if the stem is not a dictionary word.\n",
    "For example, in the English language-\n",
    "\n",
    "* beautiful and beautifully are stemmed to beauti \n",
    "* good, better and best are stemmed to good, better and best respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauti'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install stemming\n",
    "from stemming.porter2 import stem\n",
    "stem(\"beautifully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lemmatisation\n",
    "Lemmatisation is the process of reducing a group of words into their lemma or dictionary form. It takes into account things like POS(Parts of Speech), the meaning of the word in the sentence, the meaning of the word in the nearby sentences etc. before reducing the word to its lemma. For example, in the English Language-\n",
    "\n",
    "* beautiful and beautifully are lemmatised to beautiful and beautifully respectively.\n",
    "* good, better and best are lemmatised to good, good and good respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/mk194903/anaconda3/lib/python3.6/site-packages (2.0.18)\n",
      "Requirement already satisfied: regex==2018.01.10 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (2018.1.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (2.19.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (1.15.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (6.12.1)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.26.0)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
      "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 3.1MB/s ta 0:00:0121\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/mk194903/anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /Users/mk194903/anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n",
      "good good\n",
      "better better\n",
      "best good\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en\n",
    "import spacy\n",
    "nlp=spacy.load(\"en\")\n",
    "doc=\"good better best\"\n",
    "\n",
    "for token in nlp(doc):\n",
    "    print(token,token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word Embeddings\n",
    "Word Embeddings is the name of the techniques which are used to represent Natural Language in vector form of real numbers. They are useful because of computers’ inability to process Natural Language. So these Word Embeddings capture the essence and relationship between words in a Natural Language using real numbers. In Word Embeddings, a word or a phrase is represented in a fixed dimension vector of length say 100.\n",
    "\n",
    "So for example-\n",
    "\n",
    "A word “man” might be represented in a 5-dimension vector as\n",
    "<img src=\"images/word-vector.png\" alt=\"Word Vector\" />\n",
    "where each of these numbers is the magnitude of the word in a particular direction.\n",
    "<img src=\"images/Word-Vectors-direction.png\" alt=\"Word Vector\" />\n",
    "\n",
    "**Implementation:** Here is how you can obtain pre-trained Word Vector of a word using the gensim package.\n",
    "\n",
    "Download the Google News pre-trained Word Vectors from here(https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.59082031e-02,  9.22851562e-02,  1.07910156e-01,  2.83203125e-01,\n",
       "       -2.43164062e-01,  1.90429688e-02,  4.08203125e-01, -3.17382812e-02,\n",
       "       -4.78515625e-02,  6.34765625e-02, -9.32617188e-02, -4.46777344e-02,\n",
       "       -2.41210938e-01, -1.58203125e-01, -5.83496094e-02,  2.51953125e-01,\n",
       "       -3.24707031e-02,  1.00097656e-01, -4.56542969e-02,  1.35742188e-01,\n",
       "       -2.07031250e-01, -3.73046875e-01,  4.39453125e-02,  4.24804688e-02,\n",
       "        6.93359375e-02, -2.42187500e-01, -2.75390625e-01,  1.95312500e-01,\n",
       "        2.26562500e-01, -1.90429688e-01, -2.35351562e-01, -5.56640625e-02,\n",
       "       -1.25000000e-01, -8.78906250e-02, -2.33398438e-01,  9.61914062e-02,\n",
       "       -4.83398438e-02,  4.54101562e-02,  9.81445312e-02,  5.76171875e-02,\n",
       "       -4.17480469e-02,  2.02148438e-01, -9.03320312e-02,  2.75390625e-01,\n",
       "       -6.34765625e-02,  4.93164062e-02,  2.92968750e-02,  2.57812500e-01,\n",
       "        1.32812500e-01,  7.42187500e-02,  6.64062500e-02, -1.37695312e-01,\n",
       "       -1.73828125e-01,  1.89453125e-01, -1.88476562e-01, -4.34570312e-02,\n",
       "        2.11181641e-02, -3.02734375e-01,  1.76757812e-01,  3.24218750e-01,\n",
       "       -1.27563477e-02,  1.47094727e-02,  1.88476562e-01,  9.61914062e-02,\n",
       "       -8.15429688e-02, -6.64062500e-02,  9.71679688e-02,  2.29492188e-01,\n",
       "       -2.94921875e-01, -1.09375000e-01,  6.54296875e-02, -7.42187500e-02,\n",
       "       -3.41796875e-03, -6.88476562e-02, -9.86328125e-02, -1.13769531e-01,\n",
       "        1.03027344e-01,  1.39770508e-02, -7.08007812e-02, -1.81884766e-02,\n",
       "        6.49414062e-02, -3.27148438e-02,  1.04980469e-01, -7.22656250e-02,\n",
       "       -1.67236328e-02, -6.20117188e-02, -2.82287598e-03,  1.61132812e-01,\n",
       "       -5.27343750e-02,  1.30859375e-01,  1.46484375e-01,  2.81982422e-02,\n",
       "       -5.54199219e-02,  5.06591797e-03, -3.93066406e-02, -2.33154297e-02,\n",
       "        1.16699219e-01,  9.52148438e-02,  9.08203125e-02, -6.22558594e-02,\n",
       "       -1.24023438e-01,  2.44140625e-02,  1.90429688e-01, -5.49316406e-02,\n",
       "        1.68945312e-01, -1.94091797e-02, -6.88476562e-02,  4.32128906e-02,\n",
       "        1.00097656e-01,  1.70898438e-02, -1.87500000e-01, -5.43212891e-03,\n",
       "        4.46777344e-02,  1.46484375e-01, -1.19018555e-03,  8.74023438e-02,\n",
       "       -8.74023438e-02, -2.23388672e-02,  1.57226562e-01,  1.48437500e-01,\n",
       "       -1.76757812e-01, -2.47070312e-01, -2.71484375e-01,  9.13085938e-02,\n",
       "       -2.71484375e-01, -4.19921875e-02,  1.51367188e-01, -9.88769531e-03,\n",
       "        2.14843750e-01,  3.86047363e-03, -1.08886719e-01, -1.00585938e-01,\n",
       "       -3.80859375e-01,  8.00781250e-02,  8.83789062e-02, -2.06054688e-01,\n",
       "        5.49316406e-02,  6.88476562e-02,  1.40625000e-01, -1.43554688e-01,\n",
       "        9.61914062e-02, -3.46679688e-02, -1.42578125e-01,  2.44140625e-02,\n",
       "       -6.29882812e-02,  1.03027344e-01, -3.12500000e-02,  2.57812500e-01,\n",
       "        2.60009766e-02, -4.95605469e-02,  6.68945312e-02, -5.88378906e-02,\n",
       "       -1.56250000e-01,  3.10546875e-01, -3.27148438e-02, -1.75781250e-01,\n",
       "       -5.73730469e-02, -3.41796875e-01,  2.03125000e-01, -1.30859375e-01,\n",
       "       -1.01074219e-01, -3.10546875e-01,  2.44140625e-03,  6.05468750e-02,\n",
       "       -1.84570312e-01,  1.33056641e-02,  7.32421875e-02, -1.00097656e-01,\n",
       "        1.89208984e-02, -8.44726562e-02,  4.02832031e-02,  1.11328125e-01,\n",
       "        1.07421875e-01,  5.17578125e-02,  3.00781250e-01,  3.88183594e-02,\n",
       "       -1.23046875e-01, -5.29785156e-02, -1.79687500e-01, -2.18750000e-01,\n",
       "       -7.66601562e-02,  1.08886719e-01,  1.25976562e-01, -2.92968750e-01,\n",
       "       -2.77343750e-01,  1.91650391e-02, -1.92382812e-01, -3.68652344e-02,\n",
       "        1.68457031e-02,  3.16406250e-01, -1.59179688e-01,  1.20605469e-01,\n",
       "       -1.60156250e-01,  1.04492188e-01, -1.49414062e-01, -6.73828125e-02,\n",
       "        1.05468750e-01, -1.80664062e-01, -7.47070312e-02, -2.01171875e-01,\n",
       "        1.47460938e-01,  1.66015625e-02, -1.40625000e-01, -2.79541016e-02,\n",
       "       -1.60156250e-01, -4.24804688e-02, -1.20605469e-01, -7.08007812e-02,\n",
       "       -3.51562500e-01, -2.61718750e-01, -3.49609375e-01,  3.08990479e-04,\n",
       "       -1.45507812e-01, -8.93554688e-02, -3.49121094e-02,  6.83593750e-02,\n",
       "        2.49023438e-01, -7.91015625e-02, -2.99072266e-02,  3.73535156e-02,\n",
       "        2.39257812e-01,  9.08203125e-02,  1.87500000e-01, -2.81250000e-01,\n",
       "        5.49316406e-02, -1.47460938e-01,  8.25195312e-02,  7.08007812e-02,\n",
       "        8.10546875e-02,  8.74023438e-02,  3.59375000e-01,  1.09863281e-01,\n",
       "        8.31604004e-04,  5.83496094e-02, -1.46484375e-01, -2.01171875e-01,\n",
       "       -3.54003906e-02,  1.13281250e-01,  9.03320312e-02, -1.16210938e-01,\n",
       "        2.98828125e-01, -4.85229492e-03, -4.22363281e-02, -1.48437500e-01,\n",
       "        1.33789062e-01, -3.83300781e-02,  6.34765625e-02,  1.75781250e-01,\n",
       "       -6.34765625e-02, -1.57226562e-01,  1.79443359e-02,  6.62231445e-03,\n",
       "       -1.74560547e-02,  5.71289062e-02,  3.73535156e-02, -4.58984375e-01,\n",
       "       -2.58789062e-02,  1.14746094e-01,  4.76074219e-02,  3.90625000e-02,\n",
       "        8.74023438e-02, -7.27539062e-02, -1.14257812e-01,  5.71289062e-02,\n",
       "       -1.17797852e-02,  3.83300781e-02,  6.63757324e-04, -1.22070312e-03,\n",
       "       -1.38671875e-01,  1.10351562e-01, -1.01074219e-01,  6.98242188e-02,\n",
       "       -1.66015625e-01, -2.00195312e-01, -1.25000000e-01, -6.93359375e-02,\n",
       "       -1.54296875e-01,  1.09863281e-01,  3.27148438e-02,  4.83398438e-02,\n",
       "       -9.42382812e-02,  8.98437500e-02,  1.14746094e-01, -4.66308594e-02,\n",
       "       -6.29882812e-02,  1.50146484e-02, -1.55273438e-01, -1.20605469e-01,\n",
       "        2.23632812e-01,  1.99218750e-01, -2.11914062e-01, -6.54296875e-02,\n",
       "       -3.12500000e-01,  8.74023438e-02, -2.22656250e-01,  1.23046875e-01,\n",
       "        4.76074219e-03, -7.12890625e-02,  1.62109375e-01,  9.13085938e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors=KeyedVectors.load_word2vec_format('dataset/GoogleNews-vectors-negative300.bin',binary=True)\n",
    "word_vectors['human']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation:** Here is how you can train your own word vectors using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "sentence=[['first','sentence'],['second','sentence']]\n",
    "model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)\n",
    "model['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Part-Of-Speech Tagging\n",
    "In Simplistic terms, Part-Of-Speech Tagging is the process of marking up of words in a sentence as nouns, verbs, adjectives, adverbs etc. For example, in the sentence-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A DET\n",
      "look NOUN\n",
      "at ADP\n",
      "what NOUN\n",
      "lies VERB\n",
      "ahead ADV\n",
      "for ADP\n",
      "a DET\n",
      "Trump PROPN\n",
      "National PROPN\n",
      "Golf PROPN\n",
      "Club PROPN\n",
      "housekeeper NOUN\n",
      "who NOUN\n",
      "disclosed VERB\n",
      "her ADJ\n",
      "status NOUN\n",
      "as ADP\n",
      "an DET\n",
      "undocumented ADJ\n",
      "immigrant NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# POS using Spacy\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en \n",
    "nlp=spacy.load('en')\n",
    "sentence=\"A look at what lies ahead for a Trump National Golf Club housekeeper who disclosed her status as an undocumented immigrant.\"\n",
    "for token in nlp(sentence):\n",
    "   print(token,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mk194903/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/mk194903/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Automatic', 'JJ'),\n",
       " ('summarization', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('process', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('shortening', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('text', 'NN'),\n",
       " ('document', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('software', 'NN'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('order', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('create', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('summary', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('major', 'JJ'),\n",
       " ('points', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('original', 'JJ'),\n",
       " ('document', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Technologies', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('can', 'MD'),\n",
       " ('make', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('coherent', 'NN'),\n",
       " ('summary', 'JJ'),\n",
       " ('take', 'NN'),\n",
       " ('into', 'IN'),\n",
       " ('account', 'NN'),\n",
       " ('variables', 'NNS'),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('length', 'NN'),\n",
       " (',', ','),\n",
       " ('writing', 'VBG'),\n",
       " ('style', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('syntax.Automatic', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('summarization', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('part', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('data', 'NN'),\n",
       " ('mining', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('main', 'JJ'),\n",
       " ('idea', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('summarization', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('find', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('subset', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('data', 'NNS'),\n",
       " ('which', 'WDT'),\n",
       " ('contains', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('information', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('entire', 'JJ'),\n",
       " ('set', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Such', 'JJ'),\n",
       " ('techniques', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('widely', 'RB'),\n",
       " ('used', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('industry', 'NN'),\n",
       " ('today', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Search', 'NNP'),\n",
       " ('engines', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('an', 'DT'),\n",
       " ('example', 'NN'),\n",
       " (';', ':'),\n",
       " ('others', 'NNS'),\n",
       " ('include', 'VBP'),\n",
       " ('summarization', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('documents', 'NNS'),\n",
       " (',', ','),\n",
       " ('image', 'NN'),\n",
       " ('collections', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('videos', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Document', 'NNP'),\n",
       " ('summarization', 'NN'),\n",
       " ('tries', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('create', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('representative', 'JJ'),\n",
       " ('summary', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('abstract', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('entire', 'JJ'),\n",
       " ('document', 'NN'),\n",
       " (',', ','),\n",
       " ('by', 'IN'),\n",
       " ('finding', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('informative', 'JJ'),\n",
       " ('sentences', 'NNS'),\n",
       " (',', ','),\n",
       " ('while', 'IN'),\n",
       " ('in', 'IN'),\n",
       " ('image', 'NN'),\n",
       " ('summarization', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('system', 'NN'),\n",
       " ('finds', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('representative', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('important', 'JJ'),\n",
       " ('(', '('),\n",
       " ('i.e', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('salient', 'NN'),\n",
       " (')', ')'),\n",
       " ('images', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('For', 'IN'),\n",
       " ('surveillance', 'NN'),\n",
       " ('videos', 'NN'),\n",
       " (',', ','),\n",
       " ('one', 'CD'),\n",
       " ('might', 'MD'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('extract', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('important', 'JJ'),\n",
       " ('events', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('uneventful', 'JJ'),\n",
       " ('context.There', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('two', 'CD'),\n",
       " ('general', 'JJ'),\n",
       " ('approaches', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('automatic', 'JJ'),\n",
       " ('summarization', 'NN'),\n",
       " (':', ':'),\n",
       " ('extraction', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('abstraction', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Extractive', 'JJ'),\n",
       " ('methods', 'NNS'),\n",
       " ('work', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('selecting', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('subset', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('existing', 'VBG'),\n",
       " ('words', 'NNS'),\n",
       " (',', ','),\n",
       " ('phrases', 'NNS'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('sentences', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('original', 'JJ'),\n",
       " ('text', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('form', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('summary', 'NN'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('contrast', 'NN'),\n",
       " (',', ','),\n",
       " ('abstractive', 'JJ'),\n",
       " ('methods', 'NNS'),\n",
       " ('build', 'VBP'),\n",
       " ('an', 'DT'),\n",
       " ('internal', 'JJ'),\n",
       " ('semantic', 'JJ'),\n",
       " ('representation', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('then', 'RB'),\n",
       " ('use', 'VB'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('generation', 'NN'),\n",
       " ('techniques', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('create', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('summary', 'JJ'),\n",
       " ('that', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('closer', 'JJR'),\n",
       " ('to', 'TO'),\n",
       " ('what', 'WP'),\n",
       " ('a', 'DT'),\n",
       " ('human', 'JJ'),\n",
       " ('might', 'MD'),\n",
       " ('express', 'VB'),\n",
       " ('.', '.'),\n",
       " ('Such', 'PDT'),\n",
       " ('a', 'DT'),\n",
       " ('summary', 'JJ'),\n",
       " ('might', 'MD'),\n",
       " ('include', 'VB'),\n",
       " ('verbal', 'JJ'),\n",
       " ('innovations', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Research', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('date', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('focused', 'VBN'),\n",
       " ('primarily', 'RB'),\n",
       " ('on', 'IN'),\n",
       " ('extractive', 'JJ'),\n",
       " ('methods', 'NNS'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('are', 'VBP'),\n",
       " ('appropriate', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('image', 'NN'),\n",
       " ('collection', 'NN'),\n",
       " ('summarization', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('video', 'NN'),\n",
       " ('summarization', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS using NLTK\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "# Clear content POS\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Named Entity Recognition\n",
    "Named Entity Recognition is the task of identifying entities in a sentence and classifying them into categories like a person, organisation, date, location, time etc. For example, a NER would take in a sentence like –"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram \n",
      "of \n",
      "Apple ORG\n",
      "Inc. ORG\n",
      "travelled \n",
      "to \n",
      "Sydney GPE\n",
      "on \n",
      "5th DATE\n",
      "October DATE\n",
      "2017 DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en')\n",
    "sentence=\"Ram of Apple Inc. travelled to Sydney on 5th October 2017\"\n",
    "for token in nlp(sentence):\n",
    "   print(token, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sentiment Analysis\n",
    "Sentiment Analysis is a broad range of subjective analysis which uses Natural Language processing techniques to perform tasks such as identifying the sentiment of a customer review, positive or negative feeling in a sentence, judging mood via voice analysis or written text analysis etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mk194903/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['VADER is smart, handsome, and funny.',\n",
       " 'VADER is smart, handsome, and funny!',\n",
       " \"It was one of the worst movies I've seen, despite good reviews.\",\n",
       " 'Unbelievably bad acting!!',\n",
       " 'Poor direction.',\n",
       " 'VERY poor production.',\n",
       " 'The movie was bad.',\n",
       " 'Very bad movie.',\n",
       " 'VERY bad movie.',\n",
       " 'VERY BAD movie.',\n",
       " 'VERY BAD movie!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentences = [\"VADER is smart, handsome, and funny.\", # positive sentence example\n",
    "\"VADER is smart, handsome, and funny!\"] # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "paragraph = \"It was one of the worst movies I've seen, despite good reviews. \\\n",
    " Unbelievably bad acting!! Poor direction. VERY poor production. \\\n",
    " The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\"\n",
    "from nltk import tokenize\n",
    "lines_list = tokenize.sent_tokenize(paragraph)\n",
    "sentences.extend(lines_list)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is smart, handsome, and funny.\n",
      "compound: 0.8316, neg: 0.0, neu: 0.254, pos: 0.746, \n",
      "VADER is smart, handsome, and funny!\n",
      "compound: 0.8439, neg: 0.0, neu: 0.248, pos: 0.752, \n",
      "It was one of the worst movies I've seen, despite good reviews.\n",
      "compound: -0.7584, neg: 0.394, neu: 0.606, pos: 0.0, \n",
      "Unbelievably bad acting!!\n",
      "compound: -0.6572, neg: 0.686, neu: 0.314, pos: 0.0, \n",
      "Poor direction.\n",
      "compound: -0.4767, neg: 0.756, neu: 0.244, pos: 0.0, \n",
      "VERY poor production.\n",
      "compound: -0.6281, neg: 0.674, neu: 0.326, pos: 0.0, \n",
      "The movie was bad.\n",
      "compound: -0.5423, neg: 0.538, neu: 0.462, pos: 0.0, \n",
      "Very bad movie.\n",
      "compound: -0.5849, neg: 0.655, neu: 0.345, pos: 0.0, \n",
      "VERY bad movie.\n",
      "compound: -0.6732, neg: 0.694, neu: 0.306, pos: 0.0, \n",
      "VERY BAD movie.\n",
      "compound: -0.7398, neg: 0.724, neu: 0.276, pos: 0.0, \n",
      "VERY BAD movie!\n",
      "compound: -0.7616, neg: 0.735, neu: 0.265, pos: 0.0, \n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "     print(sentence)\n",
    "     ss = sid.polarity_scores(sentence)\n",
    "     for k in sorted(ss):\n",
    "         print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "     print() #negation-Contradiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Semantic Text Similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Text Similarity is the process of analysing similarity between two pieces of text with respect to the meaning and essence of the text rather than analysing the syntax of the two pieces of text. Also, similarity is different than relatedness.<br>\n",
    "Words can be similar in two ways **lexically** and **semantically**. Words are similar lexically if they have a *similar character sequence*. Words are similar semantically if they have the *same thing*, are opposite of each other, used in the same way, used in the *same context* and one is a type of another.\n",
    "<br>\n",
    "### > Lexical Similarity \n",
    "#### 1. String-Based Similarity \n",
    "Operate on string sequences and character composition. <br>\n",
    "- **Character-Based Similarity -** N-gram is a sub-sequence of n items from a given sequence of text. **N-gram** similarity algorithms compare the n-grams from each character or word in two strings. Distance is computed by dividing the number of similar n-grams by maximal number of n-grams<br>\n",
    "- **Term-based Similarity -** Most famous **Cosine similarity** is a measure of similarity between two vectors of an inner product space that measures the cosine of the angle between them.\n",
    "<br>\n",
    "\n",
    "### > Semantic Similarity \n",
    "#### 1. Corpus-Based\n",
    "Determines the similarity between words according to information gained from large corpora. A Corpus is a large collection of written or spoken texts that is used for language research.<br>\n",
    "- **Latent Semantic Analysis (LSA) -**  is the most popular Corpus-Based similarity technique. LSA assumes that words that are close in meaning will occur in similar pieces of text. A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique which called singular value decomposition (SVD) is used to reduce the number of columns while preserving the similarity structure among rows. Words are then compared by taking the cosine of the angle between the two vectors formed by any two rows.\n",
    "- **Probabilistic latent semantic analysis(pLSA) -** Probabilistic latent semantic analysis, also known as probabilistic latent semantic indexing is a statistical technique for the analysis of two-mode and co-occurrence data.\n",
    "\n",
    "#### 2. Knowledge-Based \n",
    "Determines the degree of similarity between words using information derived from semantic networks.<br>\n",
    "- **Vector -** measure creates a co–occurrence matrix for each word used in the WordNet glosses from a given corpus, and then represents each gloss/concept with a vector that is the average of these co–occurrence vectors. The most popular packages that cover knowledge-based similarity measures are **WordNet::Similarity** and **Natural Language Toolkit (NLTK)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.  Text Summarisation\n",
    "Text Summarisation is the process of shortening up of a text by identifying the important points of the text and creating a summary using these points. The goal of Text Summarisation is to retain maximum information along with maximum shortening of text without altering the meaning of the text.<br>\n",
    "Here is how you can quickly summarise your text using the gensim package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images.\\nExtractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "sentence=\"Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the information of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization.\"\n",
    "summarize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [A Survey of Text Similarity Approaches](https://pdfs.semanticscholar.org/5b5c/a878c534aee3882a038ef9e82f46e102131b.pdf)\n",
    "* [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
