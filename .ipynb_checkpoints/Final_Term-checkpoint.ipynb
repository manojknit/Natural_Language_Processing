{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What are the steps in NLP pre-processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP like any data science problem questions are answered using text. In order to run machine learning algorithms and techniques, a text should be transformed into numerical features. This whole process comes in NLP pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning\n",
    "Process of getting rid of the less useful parts of text through stopword removal, dealing with capitalization and characters and other details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Capitalization\n",
    "Text often has a variety of capitalization reflecting the beginning of sentences, proper nouns emphasis. The most common approach is to reduce everything to lower case for simplicity but it is important to remember that some words, like “US” to “us”, can change meanings when reduced to the lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Stopword\n",
    "A major portion of the words in a text are connecting parts of a sentence rather than showing subjects, objects or intent. Word like “the” or “and” cab be removed by comparing text to a list of stopword.\n",
    "\n",
    "IN:\n",
    "['He', 'did', 'not', 'try', 'to', 'navigate', 'after', 'the', 'first', 'bold', 'flight', ',', 'for', 'the', 'reaction', 'had', 'taken', 'something', 'out', 'of', 'his', 'soul', '.']\n",
    "\n",
    "OUT:\n",
    "['try', 'navigate', 'first', 'bold', 'flight', ',', 'reaction', 'taken', 'something', 'soul', '.']\n",
    "\n",
    "Sometimes we can create our own stopword dictionary manually or utilize prebuilt libraries depending on the sensitivity required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization describes splitting paragraphs into sentences, or sentences into individual words. For the former Sentence Boundary Disambiguation (SBD) can be applied to create a list of individual sentences. This relies on a pre-trained, language specific algorithms like the Punkt Models from NLTK.\n",
    "\n",
    "Most commonly this split across white spaces, for example:\n",
    "\n",
    "IN:\n",
    "\"He did not try to navigate after the first bold flight, for the reaction had taken something out of his soul.\"\n",
    "\n",
    "OUT:\n",
    "['He', 'did', 'not', 'try', 'to', 'navigate', 'after', 'the', 'first', 'bold', 'flight', ',', 'for', 'the', 'reaction', 'had', 'taken', 'something', 'out', 'of', 'his', 'soul', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Stemming\n",
    "Stemming is the process of reducing the words(generally modified or derived) to their word stem or root form. The objective of stemming is to reduce related words to the same stem even if the stem is not a dictionary word.\n",
    "For example, in the English language-\n",
    "\n",
    "* beautiful and beautifully are stemmed to beauti \n",
    "* good, better and best are stemmed to good, better and best respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauti'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install stemming\n",
    "from stemming.porter2 import stem\n",
    "stem(\"beautifully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Lemmatisation\n",
    "Lemmatisation is the process of reducing a group of words into their lemma or dictionary form. It takes into account things like POS(Parts of Speech), the meaning of the word in the sentence, the meaning of the word in the nearby sentences etc. before reducing the word to its lemma. For example, in the English Language-\n",
    "\n",
    "* beautiful and beautifully are lemmatised to beautiful and beautifully respectively.\n",
    "* good, better and best are lemmatised to good, good and good respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/mk194903/anaconda3/lib/python3.6/site-packages (2.0.18)\n",
      "Requirement already satisfied: regex==2018.01.10 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (2018.1.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (2.19.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (1.15.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (6.12.1)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.7)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.26.0)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/mk194903/anaconda3/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
      "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 3.1MB/s ta 0:00:0121\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/mk194903/anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /Users/mk194903/anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n",
      "good good\n",
      "better better\n",
      "best good\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en\n",
    "import spacy\n",
    "nlp=spacy.load(\"en\")\n",
    "doc=\"good better best\"\n",
    "\n",
    "for token in nlp(doc):\n",
    "    print(token,token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Following code converts text to lower case, removes non relevent characters, stop words. Which completes cleansing part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus cleaning\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"^b\", \"\", string)\n",
    "    string = re.sub(r\"\\\\n \", \"\", string)\n",
    "    string = re.sub(r\"\\'s\", \"\", string)\n",
    "    string = re.sub(r\"\\'ve\", \"\", string)\n",
    "    string = re.sub(r\"n\\'t\", \"\", string)\n",
    "    string = re.sub(r\"\\'re\", \"\", string)\n",
    "    string = re.sub(r\"\\'d\", \"\", string)\n",
    "    string = re.sub(r\"\\'ll\", \"\", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \"\", string)\n",
    "    string = re.sub(r\"'\", \"\", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"[0-9]\\w+|[0-9]\",\"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = ' '.join(Word(word).lemmatize() for word in string.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text\n",
    "!pip install textblob\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from textblob import Word\n",
    "df['comment_text_clean'] = df['comment_text'].apply(lambda x : clean_str(x))  # calling clean for all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Embedding/Text Vectors\n",
    "Word Embeddings is the name of the techniques which are used to represent Natural Language in vector form of real numbers. They are useful because of computers’ inability to process Natural Language. So these Word Embeddings capture the essence and relationship between words in a Natural Language using real numbers. In Word Embeddings, a word or a phrase is represented in a fixed dimension vector of length say 100. **Word2Vec** and **GloVe** are the most common models to convert text to vectors.\n",
    "\n",
    "So for example-\n",
    "\n",
    "A word “man” might be represented in a 5-dimension vector as\n",
    "<img src=\"images/word-vector.png\" alt=\"Word Vector\" />\n",
    "where each of these numbers is the magnitude of the word in a particular direction.\n",
    "<img src=\"images/Word-Vectors-direction.png\" alt=\"Word Vector\" />\n",
    "\n",
    "**Implementation:** Here is how you can obtain pre-trained Word Vector of a word using the gensim package.\n",
    "\n",
    "Download the Google News pre-trained Word Vectors from here(https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.59082031e-02,  9.22851562e-02,  1.07910156e-01,  2.83203125e-01,\n",
       "       -2.43164062e-01,  1.90429688e-02,  4.08203125e-01, -3.17382812e-02,\n",
       "       -4.78515625e-02,  6.34765625e-02, -9.32617188e-02, -4.46777344e-02,\n",
       "       -2.41210938e-01, -1.58203125e-01, -5.83496094e-02,  2.51953125e-01,\n",
       "       -3.24707031e-02,  1.00097656e-01, -4.56542969e-02,  1.35742188e-01,\n",
       "       -2.07031250e-01, -3.73046875e-01,  4.39453125e-02,  4.24804688e-02,\n",
       "        6.93359375e-02, -2.42187500e-01, -2.75390625e-01,  1.95312500e-01,\n",
       "        2.26562500e-01, -1.90429688e-01, -2.35351562e-01, -5.56640625e-02,\n",
       "       -1.25000000e-01, -8.78906250e-02, -2.33398438e-01,  9.61914062e-02,\n",
       "       -4.83398438e-02,  4.54101562e-02,  9.81445312e-02,  5.76171875e-02,\n",
       "       -4.17480469e-02,  2.02148438e-01, -9.03320312e-02,  2.75390625e-01,\n",
       "       -6.34765625e-02,  4.93164062e-02,  2.92968750e-02,  2.57812500e-01,\n",
       "        1.32812500e-01,  7.42187500e-02,  6.64062500e-02, -1.37695312e-01,\n",
       "       -1.73828125e-01,  1.89453125e-01, -1.88476562e-01, -4.34570312e-02,\n",
       "        2.11181641e-02, -3.02734375e-01,  1.76757812e-01,  3.24218750e-01,\n",
       "       -1.27563477e-02,  1.47094727e-02,  1.88476562e-01,  9.61914062e-02,\n",
       "       -8.15429688e-02, -6.64062500e-02,  9.71679688e-02,  2.29492188e-01,\n",
       "       -2.94921875e-01, -1.09375000e-01,  6.54296875e-02, -7.42187500e-02,\n",
       "       -3.41796875e-03, -6.88476562e-02, -9.86328125e-02, -1.13769531e-01,\n",
       "        1.03027344e-01,  1.39770508e-02, -7.08007812e-02, -1.81884766e-02,\n",
       "        6.49414062e-02, -3.27148438e-02,  1.04980469e-01, -7.22656250e-02,\n",
       "       -1.67236328e-02, -6.20117188e-02, -2.82287598e-03,  1.61132812e-01,\n",
       "       -5.27343750e-02,  1.30859375e-01,  1.46484375e-01,  2.81982422e-02,\n",
       "       -5.54199219e-02,  5.06591797e-03, -3.93066406e-02, -2.33154297e-02,\n",
       "        1.16699219e-01,  9.52148438e-02,  9.08203125e-02, -6.22558594e-02,\n",
       "       -1.24023438e-01,  2.44140625e-02,  1.90429688e-01, -5.49316406e-02,\n",
       "        1.68945312e-01, -1.94091797e-02, -6.88476562e-02,  4.32128906e-02,\n",
       "        1.00097656e-01,  1.70898438e-02, -1.87500000e-01, -5.43212891e-03,\n",
       "        4.46777344e-02,  1.46484375e-01, -1.19018555e-03,  8.74023438e-02,\n",
       "       -8.74023438e-02, -2.23388672e-02,  1.57226562e-01,  1.48437500e-01,\n",
       "       -1.76757812e-01, -2.47070312e-01, -2.71484375e-01,  9.13085938e-02,\n",
       "       -2.71484375e-01, -4.19921875e-02,  1.51367188e-01, -9.88769531e-03,\n",
       "        2.14843750e-01,  3.86047363e-03, -1.08886719e-01, -1.00585938e-01,\n",
       "       -3.80859375e-01,  8.00781250e-02,  8.83789062e-02, -2.06054688e-01,\n",
       "        5.49316406e-02,  6.88476562e-02,  1.40625000e-01, -1.43554688e-01,\n",
       "        9.61914062e-02, -3.46679688e-02, -1.42578125e-01,  2.44140625e-02,\n",
       "       -6.29882812e-02,  1.03027344e-01, -3.12500000e-02,  2.57812500e-01,\n",
       "        2.60009766e-02, -4.95605469e-02,  6.68945312e-02, -5.88378906e-02,\n",
       "       -1.56250000e-01,  3.10546875e-01, -3.27148438e-02, -1.75781250e-01,\n",
       "       -5.73730469e-02, -3.41796875e-01,  2.03125000e-01, -1.30859375e-01,\n",
       "       -1.01074219e-01, -3.10546875e-01,  2.44140625e-03,  6.05468750e-02,\n",
       "       -1.84570312e-01,  1.33056641e-02,  7.32421875e-02, -1.00097656e-01,\n",
       "        1.89208984e-02, -8.44726562e-02,  4.02832031e-02,  1.11328125e-01,\n",
       "        1.07421875e-01,  5.17578125e-02,  3.00781250e-01,  3.88183594e-02,\n",
       "       -1.23046875e-01, -5.29785156e-02, -1.79687500e-01, -2.18750000e-01,\n",
       "       -7.66601562e-02,  1.08886719e-01,  1.25976562e-01, -2.92968750e-01,\n",
       "       -2.77343750e-01,  1.91650391e-02, -1.92382812e-01, -3.68652344e-02,\n",
       "        1.68457031e-02,  3.16406250e-01, -1.59179688e-01,  1.20605469e-01,\n",
       "       -1.60156250e-01,  1.04492188e-01, -1.49414062e-01, -6.73828125e-02,\n",
       "        1.05468750e-01, -1.80664062e-01, -7.47070312e-02, -2.01171875e-01,\n",
       "        1.47460938e-01,  1.66015625e-02, -1.40625000e-01, -2.79541016e-02,\n",
       "       -1.60156250e-01, -4.24804688e-02, -1.20605469e-01, -7.08007812e-02,\n",
       "       -3.51562500e-01, -2.61718750e-01, -3.49609375e-01,  3.08990479e-04,\n",
       "       -1.45507812e-01, -8.93554688e-02, -3.49121094e-02,  6.83593750e-02,\n",
       "        2.49023438e-01, -7.91015625e-02, -2.99072266e-02,  3.73535156e-02,\n",
       "        2.39257812e-01,  9.08203125e-02,  1.87500000e-01, -2.81250000e-01,\n",
       "        5.49316406e-02, -1.47460938e-01,  8.25195312e-02,  7.08007812e-02,\n",
       "        8.10546875e-02,  8.74023438e-02,  3.59375000e-01,  1.09863281e-01,\n",
       "        8.31604004e-04,  5.83496094e-02, -1.46484375e-01, -2.01171875e-01,\n",
       "       -3.54003906e-02,  1.13281250e-01,  9.03320312e-02, -1.16210938e-01,\n",
       "        2.98828125e-01, -4.85229492e-03, -4.22363281e-02, -1.48437500e-01,\n",
       "        1.33789062e-01, -3.83300781e-02,  6.34765625e-02,  1.75781250e-01,\n",
       "       -6.34765625e-02, -1.57226562e-01,  1.79443359e-02,  6.62231445e-03,\n",
       "       -1.74560547e-02,  5.71289062e-02,  3.73535156e-02, -4.58984375e-01,\n",
       "       -2.58789062e-02,  1.14746094e-01,  4.76074219e-02,  3.90625000e-02,\n",
       "        8.74023438e-02, -7.27539062e-02, -1.14257812e-01,  5.71289062e-02,\n",
       "       -1.17797852e-02,  3.83300781e-02,  6.63757324e-04, -1.22070312e-03,\n",
       "       -1.38671875e-01,  1.10351562e-01, -1.01074219e-01,  6.98242188e-02,\n",
       "       -1.66015625e-01, -2.00195312e-01, -1.25000000e-01, -6.93359375e-02,\n",
       "       -1.54296875e-01,  1.09863281e-01,  3.27148438e-02,  4.83398438e-02,\n",
       "       -9.42382812e-02,  8.98437500e-02,  1.14746094e-01, -4.66308594e-02,\n",
       "       -6.29882812e-02,  1.50146484e-02, -1.55273438e-01, -1.20605469e-01,\n",
       "        2.23632812e-01,  1.99218750e-01, -2.11914062e-01, -6.54296875e-02,\n",
       "       -3.12500000e-01,  8.74023438e-02, -2.22656250e-01,  1.23046875e-01,\n",
       "        4.76074219e-03, -7.12890625e-02,  1.62109375e-01,  9.13085938e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors=KeyedVectors.load_word2vec_format('dataset/GoogleNews-vectors-negative300.bin',binary=True)\n",
    "word_vectors['human']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation:** Here is how you can train your own word vectors using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010816038\n",
      "0.0007006851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mk194903/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/mk194903/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "sentence=[['first','sentence'],['second','sentence']]\n",
    "model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)\n",
    "print(model['sentence'][0])\n",
    "print(model['sentence'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part-Of-Speech Tagging\n",
    "In Simplistic terms, Part-Of-Speech Tagging is the process of marking up of words in a sentence as nouns, verbs, adjectives, adverbs etc. For example, in the sentence-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A DET\n",
      "look NOUN\n",
      "at ADP\n",
      "what NOUN\n",
      "lies VERB\n",
      "ahead ADV\n",
      "for ADP\n",
      "a DET\n",
      "Trump PROPN\n",
      "National PROPN\n",
      "Golf PROPN\n",
      "Club PROPN\n",
      "housekeeper NOUN\n",
      "who NOUN\n",
      "disclosed VERB\n",
      "her ADJ\n",
      "status NOUN\n",
      "as ADP\n",
      "an DET\n",
      "undocumented ADJ\n",
      "immigrant NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# POS using Spacy\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en \n",
    "nlp=spacy.load('en')\n",
    "sentence=\"A look at what lies ahead for a Trump National Golf Club housekeeper who disclosed her status as an undocumented immigrant.\"\n",
    "for token in nlp(sentence):\n",
    "   print(token,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mk194903/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/mk194903/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Automatic', 'JJ'),\n",
       " ('summarization', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('process', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('shortening', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('text', 'NN'),\n",
       " ('document', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('software', 'NN'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('order', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('create', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('summary', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('major', 'JJ'),\n",
       " ('points', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('original', 'JJ'),\n",
       " ('document', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Technologies', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('can', 'MD'),\n",
       " ('make', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('coherent', 'NN'),\n",
       " ('summary', 'JJ'),\n",
       " ('take', 'NN'),\n",
       " ('into', 'IN'),\n",
       " ('account', 'NN'),\n",
       " ('variables', 'NNS'),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('length', 'NN'),\n",
       " (',', ','),\n",
       " ('writing', 'VBG'),\n",
       " ('style', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('syntax.Automatic', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('summarization', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('part', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('machine', 'NN'),\n",
       " ('learning', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('data', 'NN'),\n",
       " ('mining', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('main', 'JJ'),\n",
       " ('idea', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('summarization', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('find', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('subset', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('data', 'NNS'),\n",
       " ('which', 'WDT'),\n",
       " ('contains', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('information', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('entire', 'JJ'),\n",
       " ('set', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Such', 'JJ'),\n",
       " ('techniques', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('widely', 'RB'),\n",
       " ('used', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('industry', 'NN'),\n",
       " ('today', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Search', 'NNP'),\n",
       " ('engines', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('an', 'DT'),\n",
       " ('example', 'NN'),\n",
       " (';', ':'),\n",
       " ('others', 'NNS'),\n",
       " ('include', 'VBP'),\n",
       " ('summarization', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('documents', 'NNS'),\n",
       " (',', ','),\n",
       " ('image', 'NN'),\n",
       " ('collections', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('videos', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Document', 'NNP'),\n",
       " ('summarization', 'NN'),\n",
       " ('tries', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('create', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('representative', 'JJ'),\n",
       " ('summary', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('abstract', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('entire', 'JJ'),\n",
       " ('document', 'NN'),\n",
       " (',', ','),\n",
       " ('by', 'IN'),\n",
       " ('finding', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('informative', 'JJ'),\n",
       " ('sentences', 'NNS'),\n",
       " (',', ','),\n",
       " ('while', 'IN'),\n",
       " ('in', 'IN'),\n",
       " ('image', 'NN'),\n",
       " ('summarization', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('system', 'NN'),\n",
       " ('finds', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('representative', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('important', 'JJ'),\n",
       " ('(', '('),\n",
       " ('i.e', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('salient', 'NN'),\n",
       " (')', ')'),\n",
       " ('images', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('For', 'IN'),\n",
       " ('surveillance', 'NN'),\n",
       " ('videos', 'NN'),\n",
       " (',', ','),\n",
       " ('one', 'CD'),\n",
       " ('might', 'MD'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('extract', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('important', 'JJ'),\n",
       " ('events', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('uneventful', 'JJ'),\n",
       " ('context.There', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('two', 'CD'),\n",
       " ('general', 'JJ'),\n",
       " ('approaches', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('automatic', 'JJ'),\n",
       " ('summarization', 'NN'),\n",
       " (':', ':'),\n",
       " ('extraction', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('abstraction', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Extractive', 'JJ'),\n",
       " ('methods', 'NNS'),\n",
       " ('work', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('selecting', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('subset', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('existing', 'VBG'),\n",
       " ('words', 'NNS'),\n",
       " (',', ','),\n",
       " ('phrases', 'NNS'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('sentences', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('original', 'JJ'),\n",
       " ('text', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('form', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('summary', 'NN'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('contrast', 'NN'),\n",
       " (',', ','),\n",
       " ('abstractive', 'JJ'),\n",
       " ('methods', 'NNS'),\n",
       " ('build', 'VBP'),\n",
       " ('an', 'DT'),\n",
       " ('internal', 'JJ'),\n",
       " ('semantic', 'JJ'),\n",
       " ('representation', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('then', 'RB'),\n",
       " ('use', 'VB'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('generation', 'NN'),\n",
       " ('techniques', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('create', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('summary', 'JJ'),\n",
       " ('that', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('closer', 'JJR'),\n",
       " ('to', 'TO'),\n",
       " ('what', 'WP'),\n",
       " ('a', 'DT'),\n",
       " ('human', 'JJ'),\n",
       " ('might', 'MD'),\n",
       " ('express', 'VB'),\n",
       " ('.', '.'),\n",
       " ('Such', 'PDT'),\n",
       " ('a', 'DT'),\n",
       " ('summary', 'JJ'),\n",
       " ('might', 'MD'),\n",
       " ('include', 'VB'),\n",
       " ('verbal', 'JJ'),\n",
       " ('innovations', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Research', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('date', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('focused', 'VBN'),\n",
       " ('primarily', 'RB'),\n",
       " ('on', 'IN'),\n",
       " ('extractive', 'JJ'),\n",
       " ('methods', 'NNS'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('are', 'VBP'),\n",
       " ('appropriate', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('image', 'NN'),\n",
       " ('collection', 'NN'),\n",
       " ('summarization', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('video', 'NN'),\n",
       " ('summarization', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS using NLTK\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "# Clear content POS\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition\n",
    "Named Entity Recognition is the task of identifying entities in a sentence and classifying them into categories like a person, organisation, date, location, time etc. For example, a NER would take in a sentence like –"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram \n",
      "of \n",
      "Apple ORG\n",
      "Inc. ORG\n",
      "travelled \n",
      "to \n",
      "Sydney GPE\n",
      "on \n",
      "5th DATE\n",
      "October DATE\n",
      "2017 DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en')\n",
    "sentence=\"Ram of Apple Inc. travelled to Sydney on 5th October 2017\"\n",
    "for token in nlp(sentence):\n",
    "   print(token, token.ent_type_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What is distillation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation \n",
    "Distill the unstructured or structured data source into a \n",
    "Condensed extracted representation of the unstructured data with weightages of importance and relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentiment Analysis\n",
    "Sentiment Analysis is a broad range of subjective analysis which uses Natural Language processing techniques to perform tasks such as identifying the sentiment of a customer review, positive or negative feeling in a sentence, judging mood via voice analysis or written text analysis etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mk194903/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['VADER is smart, handsome, and funny.',\n",
       " 'VADER is smart, handsome, and funny!',\n",
       " \"It was one of the worst movies I've seen, despite good reviews.\",\n",
       " 'Unbelievably bad acting!!',\n",
       " 'Poor direction.',\n",
       " 'VERY poor production.',\n",
       " 'The movie was bad.',\n",
       " 'Very bad movie.',\n",
       " 'VERY bad movie.',\n",
       " 'VERY BAD movie.',\n",
       " 'VERY BAD movie!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentences = [\"VADER is smart, handsome, and funny.\", # positive sentence example\n",
    "\"VADER is smart, handsome, and funny!\"] # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "paragraph = \"It was one of the worst movies I've seen, despite good reviews. \\\n",
    " Unbelievably bad acting!! Poor direction. VERY poor production. \\\n",
    " The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\"\n",
    "from nltk import tokenize\n",
    "lines_list = tokenize.sent_tokenize(paragraph)\n",
    "sentences.extend(lines_list)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is smart, handsome, and funny.\n",
      "compound: 0.8316, neg: 0.0, neu: 0.254, pos: 0.746, \n",
      "VADER is smart, handsome, and funny!\n",
      "compound: 0.8439, neg: 0.0, neu: 0.248, pos: 0.752, \n",
      "It was one of the worst movies I've seen, despite good reviews.\n",
      "compound: -0.7584, neg: 0.394, neu: 0.606, pos: 0.0, \n",
      "Unbelievably bad acting!!\n",
      "compound: -0.6572, neg: 0.686, neu: 0.314, pos: 0.0, \n",
      "Poor direction.\n",
      "compound: -0.4767, neg: 0.756, neu: 0.244, pos: 0.0, \n",
      "VERY poor production.\n",
      "compound: -0.6281, neg: 0.674, neu: 0.326, pos: 0.0, \n",
      "The movie was bad.\n",
      "compound: -0.5423, neg: 0.538, neu: 0.462, pos: 0.0, \n",
      "Very bad movie.\n",
      "compound: -0.5849, neg: 0.655, neu: 0.345, pos: 0.0, \n",
      "VERY bad movie.\n",
      "compound: -0.6732, neg: 0.694, neu: 0.306, pos: 0.0, \n",
      "VERY BAD movie.\n",
      "compound: -0.7398, neg: 0.724, neu: 0.276, pos: 0.0, \n",
      "VERY BAD movie!\n",
      "compound: -0.7616, neg: 0.735, neu: 0.265, pos: 0.0, \n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "     print(sentence)\n",
    "     ss = sid.polarity_scores(sentence)\n",
    "     for k in sorted(ss):\n",
    "         print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "     print() #negation-Contradiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Text Similarity is the process of analysing similarity between two pieces of text with respect to the meaning and essence of the text rather than analysing the syntax of the two pieces of text. Also, similarity is different than relatedness.<br>\n",
    "Words can be similar in two ways **lexically** and **semantically**. Words are similar lexically if they have a *similar character sequence*. Words are similar semantically if they have the *same thing*, are opposite of each other, used in the same way, used in the *same context* and one is a type of another.\n",
    "<br>\n",
    "### > Lexical Similarity \n",
    "#### 1. String-Based Similarity \n",
    "Operate on string sequences and character composition. <br>\n",
    "- **Character-Based Similarity -** N-gram is a sub-sequence of n items from a given sequence of text. **N-gram** similarity algorithms compare the n-grams from each character or word in two strings. Distance is computed by dividing the number of similar n-grams by maximal number of n-grams<br>\n",
    "- **Term-based Similarity -** Most famous **Cosine similarity** is a measure of similarity between two vectors of an inner product space that measures the cosine of the angle between them.\n",
    "<br>\n",
    "\n",
    "### > Semantic Similarity \n",
    "#### 1. Corpus-Based\n",
    "Determines the similarity between words according to information gained from large corpora. A Corpus is a large collection of written or spoken texts that is used for language research.<br>\n",
    "- **Latent Semantic Analysis (LSA) -**  is the most popular Corpus-Based similarity technique. LSA assumes that words that are close in meaning will occur in similar pieces of text. A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique which called singular value decomposition (SVD) is used to reduce the number of columns while preserving the similarity structure among rows. Words are then compared by taking the cosine of the angle between the two vectors formed by any two rows.\n",
    "- **Probabilistic latent semantic analysis(pLSA) -** Probabilistic latent semantic analysis, also known as probabilistic latent semantic indexing is a statistical technique for the analysis of two-mode and co-occurrence data.\n",
    "\n",
    "#### 2. Knowledge-Based \n",
    "Determines the degree of similarity between words using information derived from semantic networks.<br>\n",
    "- **Vector -** measure creates a co–occurrence matrix for each word used in the WordNet glosses from a given corpus, and then represents each gloss/concept with a vector that is the average of these co–occurrence vectors. The most popular packages that cover knowledge-based similarity measures are **WordNet::Similarity** and **Natural Language Toolkit (NLTK)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Text Summarisation\n",
    "Text Summarisation is the process of shortening up of a text by identifying the important points of the text and creating a summary using these points. The goal of Text Summarisation is to retain maximum information along with maximum shortening of text without altering the meaning of the text.<br>\n",
    "Here is how you can quickly summarise your text using the gensim package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images.\\nExtractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "sentence=\"Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the information of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization.\"\n",
    "summarize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Count / Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps one of the more basic tools for feature engineering, adding word count, sentence count, punctuation counts and Industry specific word counts can greatly help in prediction or classification. \n",
    "\n",
    "![Word Count](images/word-count-density.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Topic Modeling\n",
    "**Latent Dirichlet Allocation (LDA)** is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Author/Sender Importance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author, sender or writer of the text can sldo be important value add to feature list. Importance can be ranked in numeric range like (1-10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Severity, Priority, Urgency, Importance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on context Severity, Priority, Urgency, and Importance can be a great addition to feature list in order to feed into machine learning algorithms.Importance can be ranked in numeric range like (1-10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Context "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some context can decide importance like historical, science, etc. So these contexts can be labeled. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Describe the difference between CountVectorizer, tf-idf and word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "Word Embeddings are the text converted into numbers. There might be different numerical representations of the same text as per embedding technique used.\n",
    "Formally we can say, A Word Embedding format tries to convert a word to a vector by mapping it with a dictionary.\n",
    "\n",
    "Example for simple word embedding.\n",
    "\n",
    "Sentence=” Word Embeddings are Word converted into numbers ”<br>\n",
    "A dictionary can be the list of all unique words in the sentence.<br>\n",
    "Dictionary = [‘Word’,’Embeddings’,’are’,’Converted’,’into’,’numbers’]<br>\n",
    "\n",
    "Let's represent word 'Embeddings' in vector form<br>\n",
    "[0,1,0,0,0,0]\n",
    "\n",
    "### Types of Word Embeddings\n",
    "1. Frequency based Embedding\n",
    "    * Count Vector\n",
    "    * TF-IDF Vector\n",
    "2. Prediction based Embedding\n",
    "    * CBOW (Continuous Bag of words)\n",
    "    * Skip – Gram model\n",
    "3. Pre-trained word vectors\n",
    "    * Word2Vec\n",
    "4. Training own word vectors\n",
    "\n",
    "I will analyze Count Vector, TF-IDF Vector as per ask.\n",
    "\n",
    "### Count Vector\n",
    "CountVectorizer just counts the occurrences of each word in its vocabulary. It can lowercase letters, disregard punctuation and stopwords, but it can't LEMMATIZE or STEM.\n",
    "\n",
    "Let us understand this using a simple example.\n",
    "\n",
    "D1: He is a lazy boy. She is also lazy.\n",
    "\n",
    "D2: Neel is a lazy person.\n",
    "\n",
    "The dictionary created may be a list of unique tokens(words) in the corpus =[‘He’,’She’,’lazy’,’boy’,’Neel’,’person’]\n",
    "\n",
    "Here, D=2, N=6\n",
    "\n",
    "The count matrix M of size 2 X 6 will be represented as –\n",
    "\n",
    " | He | She | lazy | boy | Neeraj | person | \n",
    " | --- | --- | --- | --- | --- | --- | \n",
    " | D1 | 1 | 1 | 2 | 1 | 0 | 0 | \n",
    " | D2 | 0 | 0 | 1 | 0 | 1 | 1 | \n",
    "\n",
    "\n",
    "sklearn CountVectorizer implements both tokenization and occurrence counting in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:\n",
      "{'smile': 2, 'perfect': 0, 'sang': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smile</th>\n",
       "      <th>perfect</th>\n",
       "      <th>sang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   smile  perfect  sang\n",
       "0      1        0     1\n",
       "1      1        0     1\n",
       "2      0        1     0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "txt = ['His smile was not perfect', 'His smile was not not not not perfect.', 'she not sang']\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "# Transforms the data into a bag of words\n",
    "count_train = count_vec.fit(txt) #text can be corpus text.tolist()\n",
    "bag_of_words = count_vec.transform(txt) # can also use fit_transform(text) # vector features\n",
    "\n",
    "print(\"vocabulary:\\n{}\".format(count_vec.vocabulary_))\n",
    "# Print the first 10 features of the count_vec\n",
    "#print(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))\n",
    "#print(\"\\nEvery 3rd feature:\\n{}\".format(count_vec.get_feature_names()[::3]))\n",
    "\n",
    "textmatrix = pd.DataFrame(bag_of_words.toarray(),columns=count_vec.vocabulary_)\n",
    "textmatrix.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "tf-idf minimizes the impact of tokens that occur very frequently across documents and that are consequently experimentally less informative than features that occur in a small fraction of the training corpus. \n",
    "#### *tf-idf(d, t) = tf(t) * idf(d, t)\n",
    "* tf(t)= the term frequency is the number of times the term appears in the document\n",
    "* idf(d, t) = the document frequency is the number of documents 'd' that contain term 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>his</th>\n",
       "      <th>smile</th>\n",
       "      <th>was</th>\n",
       "      <th>not</th>\n",
       "      <th>perfect</th>\n",
       "      <th>she</th>\n",
       "      <th>sang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.405465</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        his  smile       was       not   perfect       she      sang\n",
       "0  1.405465    1.0  1.405465  0.000000  0.000000  1.405465  1.405465\n",
       "1  1.405465    4.0  1.405465  0.000000  0.000000  1.405465  1.405465\n",
       "2  0.000000    1.0  0.000000  2.098612  2.098612  0.000000  0.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "txt1 = ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']\n",
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "txt_fitted = tf.fit(txt1)\n",
    "txt_transformed = txt_fitted.transform(txt1) # vector features\n",
    "\n",
    "#pd.DataFrame(txt_transformed.toarray()).head(15)\n",
    "textmatrix = pd.DataFrame(txt_transformed.toarray(),columns=tf.vocabulary_)\n",
    "textmatrix.head(5)\n",
    "#print (\"The text: \", txt_transformed.todense())\n",
    "#print(\"Every feature:\\n{}\".format(tf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW (Continuous Bag of words)\n",
    "The way CBOW work is that it tends to predict the probability of a word given a context. A context may be a single word or a group of words. \n",
    "\n",
    "Pros:<br>\n",
    "* Being probabilistic is nature, it is supposed to perform superior to deterministic methods(generally).\n",
    "* It is low on memory. It does not need to have huge RAM requirements like that of co-occurrence matrix where it needs to store three huge matrices.\n",
    "\n",
    "Cons:<br>\n",
    "* CBOW takes the average of the context of a word (as seen above in calculation of hidden activation). For example, Apple can be both a fruit and a company but CBOW takes an average of both the contexts and places it in between a cluster for fruits and companies.\n",
    "* Training a CBOW from scratch can take forever if not properly optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Word2vec is a group of related models that are used to produce word embeddings using mostly pretraind models. In following example we are using [google’s pre-trained model](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). It contains word vectors for a vocabulary of 3 million words trained on around 100 billion words from the google news dataset. Size 1.5 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01,\n",
       "       -8.44726562e-02,  5.73730469e-02,  5.85937500e-02, -8.25195312e-02,\n",
       "       -1.53808594e-02, -6.34765625e-02,  1.79687500e-01, -4.23828125e-01,\n",
       "       -2.25830078e-02, -1.66015625e-01, -2.51464844e-02,  1.07421875e-01,\n",
       "       -1.99218750e-01,  1.59179688e-01, -1.87500000e-01, -1.20117188e-01,\n",
       "        1.55273438e-01, -9.91210938e-02,  1.42578125e-01, -1.64062500e-01,\n",
       "       -8.93554688e-02,  2.00195312e-01, -1.49414062e-01,  3.20312500e-01,\n",
       "        3.28125000e-01,  2.44140625e-02, -9.71679688e-02, -8.20312500e-02,\n",
       "       -3.63769531e-02, -8.59375000e-02, -9.86328125e-02,  7.78198242e-03,\n",
       "       -1.34277344e-02,  5.27343750e-02,  1.48437500e-01,  3.33984375e-01,\n",
       "        1.66015625e-02, -2.12890625e-01, -1.50756836e-02,  5.24902344e-02,\n",
       "       -1.07421875e-01, -8.88671875e-02,  2.49023438e-01, -7.03125000e-02,\n",
       "       -1.59912109e-02,  7.56835938e-02, -7.03125000e-02,  1.19140625e-01,\n",
       "        2.29492188e-01,  1.41601562e-02,  1.15234375e-01,  7.50732422e-03,\n",
       "        2.75390625e-01, -2.44140625e-01,  2.96875000e-01,  3.49121094e-02,\n",
       "        2.42187500e-01,  1.35742188e-01,  1.42578125e-01,  1.75781250e-02,\n",
       "        2.92968750e-02, -1.21582031e-01,  2.28271484e-02, -4.76074219e-02,\n",
       "       -1.55273438e-01,  3.14331055e-03,  3.45703125e-01,  1.22558594e-01,\n",
       "       -1.95312500e-01,  8.10546875e-02, -6.83593750e-02, -1.47094727e-02,\n",
       "        2.14843750e-01, -1.21093750e-01,  1.57226562e-01, -2.07031250e-01,\n",
       "        1.36718750e-01, -1.29882812e-01,  5.29785156e-02, -2.71484375e-01,\n",
       "       -2.98828125e-01, -1.84570312e-01, -2.29492188e-01,  1.19140625e-01,\n",
       "        1.53198242e-02, -2.61718750e-01, -1.23046875e-01, -1.86767578e-02,\n",
       "       -6.49414062e-02, -8.15429688e-02,  7.86132812e-02, -3.53515625e-01,\n",
       "        5.24902344e-02, -2.45361328e-02, -5.43212891e-03, -2.08984375e-01,\n",
       "       -2.10937500e-01, -1.79687500e-01,  2.42187500e-01,  2.57812500e-01,\n",
       "        1.37695312e-01, -2.10937500e-01, -2.17285156e-02, -1.38671875e-01,\n",
       "        1.84326172e-02, -1.23901367e-02, -1.59179688e-01,  1.61132812e-01,\n",
       "        2.08007812e-01,  1.03027344e-01,  9.81445312e-02, -6.83593750e-02,\n",
       "       -8.72802734e-03, -2.89062500e-01, -2.14843750e-01, -1.14257812e-01,\n",
       "       -2.21679688e-01,  4.12597656e-02, -3.12500000e-01, -5.59082031e-02,\n",
       "       -9.76562500e-02,  5.81054688e-02, -4.05273438e-02, -1.73828125e-01,\n",
       "        1.64062500e-01, -2.53906250e-01, -1.54296875e-01, -2.31933594e-02,\n",
       "       -2.38281250e-01,  2.07519531e-02, -2.73437500e-01,  3.90625000e-03,\n",
       "        1.13769531e-01, -1.73828125e-01,  2.57812500e-01,  2.35351562e-01,\n",
       "        5.22460938e-02,  6.83593750e-02, -1.75781250e-01,  1.60156250e-01,\n",
       "       -5.98907471e-04,  5.98144531e-02, -2.11914062e-01, -5.54199219e-02,\n",
       "       -7.51953125e-02, -3.06640625e-01,  4.27734375e-01,  5.32226562e-02,\n",
       "       -2.08984375e-01, -5.71289062e-02, -2.09960938e-01,  3.29589844e-02,\n",
       "        1.05468750e-01, -1.50390625e-01, -9.37500000e-02,  1.16699219e-01,\n",
       "        6.44531250e-02,  2.80761719e-02,  2.41210938e-01, -1.25976562e-01,\n",
       "       -1.00585938e-01, -1.22680664e-02, -3.26156616e-04,  1.58691406e-02,\n",
       "        1.27929688e-01, -3.32031250e-02,  4.07714844e-02, -1.31835938e-01,\n",
       "        9.81445312e-02,  1.74804688e-01, -2.36328125e-01,  5.17578125e-02,\n",
       "        1.83593750e-01,  2.42919922e-02, -4.31640625e-01,  2.46093750e-01,\n",
       "       -3.03955078e-02, -2.47802734e-02, -1.17187500e-01,  1.61132812e-01,\n",
       "       -5.71289062e-02,  1.16577148e-02,  2.81250000e-01,  4.27734375e-01,\n",
       "        4.56542969e-02,  1.01074219e-01, -3.95507812e-02,  1.77001953e-02,\n",
       "       -8.98437500e-02,  1.35742188e-01,  2.08007812e-01,  1.88476562e-01,\n",
       "       -1.52343750e-01, -2.37304688e-01, -1.90429688e-01,  7.12890625e-02,\n",
       "       -2.46093750e-01, -2.61718750e-01, -2.34375000e-01, -1.45507812e-01,\n",
       "       -1.17187500e-02, -1.50390625e-01, -1.13281250e-01,  1.82617188e-01,\n",
       "        2.63671875e-01, -1.37695312e-01, -4.58984375e-01, -4.68750000e-02,\n",
       "       -1.26953125e-01, -4.22363281e-02, -1.66992188e-01,  1.26953125e-01,\n",
       "        2.59765625e-01, -2.44140625e-01, -2.19726562e-01, -8.69140625e-02,\n",
       "        1.59179688e-01, -3.78417969e-02,  8.97216797e-03, -2.77343750e-01,\n",
       "       -1.04980469e-01, -1.75781250e-01,  2.28515625e-01, -2.70996094e-02,\n",
       "        2.85156250e-01, -2.73437500e-01,  1.61132812e-02,  5.90820312e-02,\n",
       "       -2.39257812e-01,  1.77734375e-01, -1.34765625e-01,  1.38671875e-01,\n",
       "        3.53515625e-01,  1.22070312e-01,  1.43554688e-01,  9.22851562e-02,\n",
       "        2.29492188e-01, -3.00781250e-01, -4.88281250e-02, -1.79687500e-01,\n",
       "        2.96875000e-01,  1.75781250e-01,  4.80957031e-02, -3.38745117e-03,\n",
       "        7.91015625e-02, -2.38281250e-01, -2.31445312e-01,  1.66015625e-01,\n",
       "       -2.13867188e-01, -7.03125000e-02, -7.56835938e-02,  1.96289062e-01,\n",
       "       -1.29882812e-01, -1.05957031e-01, -3.53515625e-01, -1.16699219e-01,\n",
       "       -5.10253906e-02,  3.39355469e-02, -1.43554688e-01, -3.90625000e-03,\n",
       "        1.73828125e-01, -9.96093750e-02, -1.66015625e-01, -8.54492188e-02,\n",
       "       -3.82812500e-01,  5.90820312e-02, -6.22558594e-02,  8.83789062e-02,\n",
       "       -8.88671875e-02,  3.28125000e-01,  6.83593750e-02, -1.91406250e-01,\n",
       "       -8.35418701e-04,  1.04003906e-01,  1.52343750e-01, -1.53350830e-03,\n",
       "        4.16015625e-01, -3.32031250e-02,  1.49414062e-01,  2.42187500e-01,\n",
       "       -1.76757812e-01, -4.93164062e-02, -1.24511719e-01,  1.25976562e-01,\n",
       "        1.74804688e-01,  2.81250000e-01, -1.80664062e-01,  1.03027344e-01,\n",
       "       -2.75390625e-01,  2.61718750e-01,  2.46093750e-01, -4.71191406e-02,\n",
       "        6.25000000e-02,  4.16015625e-01, -3.55468750e-01,  2.22656250e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained model\n",
    "from gensim.models import KeyedVectors # Deprecated Word2Vec\n",
    "\n",
    "word_vectors_model = KeyedVectors.load_word2vec_format('dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# getting word vectors of a word\n",
    "word_vectors_model['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What are latent variables and manifolds ? Why are they important ? Give at least two examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent variables\n",
    "1.\tA latent variable is a variable which you can’t observe neither in training nor in test phase.\n",
    "2.\tIn Latent variable model, we assume that the distribution in data space is due to the influence of a small number of latent variables. We then map the latent space onto data space. It is optimized using a maximum likelihood criterion using the EM algorithm.\n",
    "3.\tIn Latent Variable modelling, Bayes’ theorem explains the distribution of data point was due to a point in latent space.\n",
    "4.\tLatent Variable modelling is also called as factor analysis. PCA is a special case of factor analysis.\n",
    "5.\tDimensionality reduction, the reverse is done data space is mapped to latent space.\n",
    "6. Latent features are computed from observed features using matrix factorization.\n",
    "7. An example would be text document analysis. 'words' extracted from the documents are features.\n",
    "8. If you factorize the data of words you can find 'topics', where 'topic' is a group of words with semantic relevance.\n",
    "\n",
    "## Latent Manifold\n",
    "The Latent manifold is the set of all Latent variables that make up the Latent Space.\n",
    "\n",
    "#### Why?\n",
    "Latent variables/manifold provide condensed meaningful important information in the form of feature which plays an essential role in prediction or classification.\n",
    "\n",
    "#### Example:\n",
    "1. In fake news project, author name is transformed to rank considering different reputation factors. Domain of news source is also converted to domain rank using external sources. These features worked as a latent variable, and combined reliability rating has been appeared as a latent variable manifold. \n",
    "\n",
    "2. In Housing price prediction project 'location' is a facor that influence house price significently, but location is not present in data set. Like distance from super market, school district, hospital, freeway etc. So location becomes latent variable. Number of landmark locations and neighborhood rating also become latent variable and formed neighborhood latent manifold.\n",
    "\n",
    "2. For example in NLP features (words) like [sail-boat, schooner, yatch, steamer, cruiser] which would 'factorize' to latent feature (topic) like 'ship' and 'boat'.<br>\n",
    "    * [sail-boat, schooner, yatch, steamer, cruiser, ...] ->[ship, boat]<br>\n",
    "    * The underlying idea is that latent features are semantically relevant 'aggregates' of observed features. When you have large-scale, high-dimensional, and noisy observed features, it makes sense to build your classifier on latent features.\n",
    "    * Ship, Boat as combined become Latent manifold."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Describe the Scikit-learn Pipeline function with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn's pipeline functionality makes easier to repeat commonly occuring steps in your modeling process. \n",
    "\n",
    "In following example pipeline gives a single interface for all 3 most common steps of transformation and resulting estimator. It encapsulates transformers and predictors inside, and then you can do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\n    vect = CountVectorizer()\\n    tfidf = TfidfTransformer()\\n    clf = SGDClassifier()\\n\\n    vX = vect.fit_transform(Xtrain)\\n    tfidfX = tfidf.fit_transform(vX)\\n    predicted = clf.fit_predict(tfidfX)\\n\\n    # Now evaluate all steps on test set\\n    vX = vect.fit_transform(Xtest)\\n    tfidfX = tfidf.fit_transform(vX)\\n    predicted = clf.fit_predict(tfidfX)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''   \n",
    "    vect = CountVectorizer()\n",
    "    tfidf = TfidfTransformer()\n",
    "    clf = SGDClassifier()\n",
    "\n",
    "    vX = vect.fit_transform(Xtrain)\n",
    "    tfidfX = tfidf.fit_transform(vX)\n",
    "    predicted = clf.fit_predict(tfidfX)\n",
    "\n",
    "    # Now evaluate all steps on test set\n",
    "    vX = vect.fit_transform(Xtest)\n",
    "    tfidfX = tfidf.fit_transform(vX)\n",
    "    predicted = clf.fit_predict(tfidfX)\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npipeline = Pipeline([\\n    ('vect', CountVectorizer()),\\n    ('tfidf', TfidfTransformer()),\\n    ('clf', SGDClassifier()),\\n])\\npredicted = pipeline.fit(Xtrain).predict(Xtrain)\\n# Now evaluate all steps on test set\\npredicted = pipeline.predict(Xtest)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With pipeline\n",
    "'''\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "predicted = pipeline.fit(Xtrain).predict(Xtrain)\n",
    "# Now evaluate all steps on test set\n",
    "predicted = pipeline.predict(Xtest)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Compare Decision Trees , Random Forests and SVM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "A decision tree is a graphical representation of all the possible solutions to a decision based on certain conditions. It's called a decision tree because it starts with a single box (or root), which then branches off into a number of solutions, just like a tree.\n",
    "![Decision Trees](images/DecisionTree.png)\n",
    "Credit: [Source](https://www.quora.com/What-is-the-difference-between-random-forest-and-decision-trees)\n",
    "#### Pros\n",
    "1. **Easy to Understand**: Decision tree output is very easy to understand even for people from the non-analytical background. It does not require any statistical knowledge to read and interpret them. Its graphical representation is very intuitive and users can easily relate their hypothesis.\n",
    "2. **Useful in Data exploration:** Decision tree is one of the fastest ways to identify the most significant variables and the relation between two or more variables. With the help of decision trees, we can create new variables/features that have better power to predict the target variable. It can also be used in the data exploration stage. For example, we are working on a problem where we have information available in hundreds of variables, their decision tree will help to identify the most significant variable.\n",
    "3. **Less data cleaning required:** It requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.\n",
    "4. **Data type is not a constraint:** It can handle both numerical and categorical variables.\n",
    "5. **Non Parametric Method:** Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.\n",
    "\n",
    "#### Cons\n",
    "1. **Overfitting:** Overfitting is one of the most practical difficulties for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detail below).\n",
    "2. **Not fit for continuous variables**: While working with continuous numerical variables, decision tree loses information when it categorizes variables in different categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Import Library\\n#Import other necessary libraries like pandas, numpy...\\nfrom sklearn import tree\\n#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\\n# Create tree object \\nmodel = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  \\n# model = tree.DecisionTreeRegressor() for regression\\n# Train the model using the training sets and check score\\nmodel.fit(X, y)\\nmodel.score(X, y)\\n#Predict Output\\npredicted= model.predict(x_test)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Import Library\n",
    "#Import other necessary libraries like pandas, numpy...\n",
    "from sklearn import tree\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create tree object \n",
    "model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  \n",
    "# model = tree.DecisionTreeRegressor() for regression\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X, y)\n",
    "model.score(X, y)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n",
    "![randomforest](images/RandomForest.png)\n",
    "#### Pros\n",
    "\n",
    "- This algorithm can solve both type of problems i.e. classification and regression and does a decent estimation at both fronts.\n",
    "- One of benefits of Random forest which excites me most is, the power of handle large data set with higher dimensionality. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. Further, the model outputs **Importance of variable,** which can be a very handy feature (on some random data set).  \n",
    "- It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "- It has methods for balancing errors in data sets where classes are imbalanced.\n",
    "- The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
    "- Random Forest involves sampling of the input data with replacement called as bootstrap sampling. Here one third of the data is not used for training and can be used to testing. These are called the **out of bag** samples. Error estimated on these out of bag samples is known as _out of bag error_. Study of error estimates by Out of bag, gives evidence to show that the out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.\n",
    "\n",
    "#### Cons\n",
    "\n",
    "- It surely does a good job at classification but not as good as for regression problem as it does not give precise continuous nature predictions. In case of regression, it doesn’t predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy.\n",
    "- Random Forest can feel like a black box approach for statistical modelers – you have very little control on what the model does. You can at best – try different parameters and random seeds!\n",
    "\n",
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Import Library\n",
    "from sklearn.ensemble import RandomForestClassifier #use RandomForestRegressor for regression problem\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create Random Forest object\n",
    "model= RandomForestClassifier(n_estimators=1000)\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X, y)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine(SVM)\n",
    "A Support Vector Machine (SVM) is a classifier that tries to maximize the margin between training data and the classification boundary (the separating hyperplane defined by 𝑋𝛽 = 0).\n",
    "![svm](images/svm.png)\n",
    "\n",
    "#### Pros:\n",
    "* It works really well with clear margin of separation\n",
    "* It is effective in high dimensional spaces.\n",
    "* It is effective in cases where number of dimensions is greater than the number of samples.\n",
    "* It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "#### Cons:\n",
    "* It doesn’t perform well, when we have large data set because the required training time is higher\n",
    "* It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping\n",
    "* SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. It is related SVC method of Python scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Import Library\\nfrom sklearn import svm\\n#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\\n# Create SVM classification object \\nmodel = svm.svc(kernel='linear', c=1, gamma=1) \\n# there is various option associated with it, like changing kernel, gamma and C value. Will discuss more # about it in next section.Train the model using the training sets and check score\\nmodel.fit(X, y)\\nmodel.score(X, y)\\n#Predict Output\\npredicted= model.predict(x_test)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Import Library\n",
    "from sklearn import svm\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create SVM classification object \n",
    "model = svm.svc(kernel='linear', c=1, gamma=1) \n",
    "# there is various option associated with it, like changing kernel, gamma and C value. Will discuss more # about it in next section.Train the model using the training sets and check score\n",
    "model.fit(X, y)\n",
    "model.score(X, y)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side by side comparision\n",
    "![df-rf-svm](images/df-rf-svm.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: How does Logistic Regression compare to svm? What if we have an svm with only binary classifications ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side by Side Compare\n",
    "\n",
    "| no. | Logistic Regression | Support Vector Machine(SVM) |\n",
    "| --- | --- | --- |\n",
    "| 1 | Logistic Regression fits the data points as if they are along a continuous function. | SVM fits a function (hyperplane) that attempts to separate two classes of data that could be of multiple dimensions. |\n",
    "| 2 | This isn't always the case for single-class classification, and so the function may have trouble classifying where P = 0.5 | SVM could have difficulty when the classes are not separable or there is not enough margin to fit a (n_dimensions - 1) hyperplane between the two classes. |\n",
    "| 3 | Generally resistant to overfitting. The less is parameters count there are less chances of overfitting. | Not sensitive to overfitting |\n",
    "| 4 | Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. | The idea is that maximizing the margin maximizes the chance that classification will be correct on new data. We assume the new data of each class is near the training data of that type. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with binary classification\n",
    "\n",
    "SVMs (linear or otherwise) inherently do binary classification. Although, there are several ways to extend to multiclass problems. The most common procedure involves transforming the problem set into a set of binary classification problems, using following two strategies:\n",
    "\n",
    "* One vs. the rest - For n classes, n binary classifiers are trained. Each predicts whether an example belongs to its 'own' class versus other class. The classifier with the largest output is taken to be the class of the example.\n",
    "* One vs. one. A binary classifier can be trained for each pair of classes. A voting procedure is used to combine the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Describe naive bayes with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Naive Bayes is used for several purposes successfully however it works specifically well with natural language processing (NLP) problems. Naive Bayes is a family of probabilistic algorithms that take advantage of probability theory and Bayes' Theorem to predict the tag of a text like a piece of news or a customer feedback.\n",
    "\n",
    "As it is probabilistic, so it calculates the probability of each tag for a given document, and then output the tag with the highest probability. It gets probabilities by using Bayes’ Theorem, which describes the probability of a feature, based on prior knowledge of conditions that might be related to that feature.\n",
    "\n",
    "In the following example will walk you through Multinomial Naive Bayes with NLP example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:\n",
    "\n",
    "| no. | Text | Tag |\n",
    "| --- | --- | --- |\n",
    "| 1 | A great game | Sports |\n",
    "| 2 | The election was over | Not Sports |\n",
    "| 3 | Very clean match | Sports |\n",
    "| 4 | A clean but forgettable game | Sports |\n",
    "| 5 | It was a close election | Not Sports |\n",
    "\n",
    "Naive Bayes is a probability base classifier, we will calculate the probability that the sentence “A very close game” is Sports, and the probability that it’s Not Sports.\n",
    "\n",
    "#### Feature Engineering\n",
    "First we need to extract features from text so we can apply models. Feature is the pieces of information that we extract from the text. As we know model accepts numbers to calculate, so we need to convert features into numbers. One basic way of doing this is **word frequencies**.\n",
    "\n",
    "We will treat each text(document) as a set of words and counts.\n",
    "\n",
    "#### Bayes’ Theorem\n",
    "![Bayes-theorem](images/Bayes-theorem.png)\n",
    "\n",
    "Lets calculate probablity of Sports or Not Sport for given text “A very close game”.\n",
    "![probability](images/nb-probability.svg)\n",
    "As we are looking just for higher probability in order to compare so we can drop common factors.\n",
    "![probability compare](images/nb-probability-compare.png)\n",
    "\n",
    "Still problem is there “A very close game” does not appears in the Sports tag texts. In this Naive can help.\n",
    "\n",
    "#### Naive\n",
    "We are treating text as set of words. We can write probabilities in terms of words with little assumption.\n",
    "nb-wordset\n",
    "![wordset](images/nb-wordset.svg)\n",
    "![wordset-prob](images/nb-wordset-prob.svg)\n",
    "\n",
    "#### Calculation\n",
    "Probability of each tag:<br>\n",
    "P(Sports) = ⅗<br>\n",
    "P(Not Sports) = ⅖<br>\n",
    "\n",
    "Calculating words probability<br>\n",
    "P(game | Sports) = (count word “game” appears in Sports tag texts) / (total number of words in sports tag) => 2/11<br>\n",
    "P(close | Sports) = 0 as close does not appear in the text.<br>\n",
    "It can be problem as we need to multiply word probabilities to get text(document) probability.<br>\n",
    "Solution is **Laplace smoothing** we add 1 to every count so it’s never zero. To balance this, we add the number of possible words to the divisor, so the division will never be greater than 1. \n",
    "\n",
    "Possible words are ['a', 'great', 'very', 'over', 'it', 'but', 'game', 'election', 'clean', 'close', 'the', 'was', 'forgettable', 'match'].\n",
    "\n",
    "Since the number of possible words is 14, applying smoothing we get that \n",
    "\n",
    "P(game | sports) = (2 + 1)/(11 + 14). \n",
    "\n",
    "The full results are:\n",
    "![nb-classifier](images/nb-classifier.png)\n",
    "\n",
    "#### Result: Classifier gives “A very close game” the Sports tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes model accuracy(in %): 95.0\n"
     ]
    }
   ],
   "source": [
    "# load the iris dataset \n",
    "from sklearn.datasets import load_iris \n",
    "iris = load_iris() \n",
    "\n",
    "# store the feature matrix (X) and response vector (y) \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "\n",
    "# splitting X and y into training and testing sets \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
    "\n",
    "# training the model on training set \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "gnb = GaussianNB() \n",
    "gnb.fit(X_train, y_train) \n",
    "\n",
    "# making predictions on the testing set \n",
    "y_pred = gnb.predict(X_test) \n",
    "\n",
    "# comparing actual response values (y_test) with predicted response values (y_pred) \n",
    "from sklearn import metrics \n",
    "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What is deep learning? Name three activation functions ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "Deep learning is a subset of AI and machine learning that uses multi-layered artificial neural networks to deliver state-of-the-art accuracy in tasks such as object detection, speech recognition, language translation and others.\n",
    "<br>\n",
    "![deeplearning](images/deeplearning.png)\n",
    "\n",
    "Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example.<br>\n",
    "In deep learning, a computer model learns to perform classification tasks directly from images, text, or sound. Deep learning models can achieve state-of-the-art accuracy, sometimes exceeding human-level performance. Models are trained by using a large set of labeled data and neural network architectures that contain many layers.\n",
    "#### How Deep Learning Works\n",
    "Most deep learning methods use neural network architectures, which is why deep learning models are often referred to as deep neural networks.\n",
    "\n",
    "The term “deep” usually refers to the number of hidden layers in the neural network. Traditional neural networks only contain 2-3 hidden layers, while deep networks can have as many as 150.\n",
    "\n",
    "Deep learning models are trained by using large sets of labeled data and neural network architectures that learn features directly from the data without the need for manual feature extraction.\n",
    "\n",
    "#### Use Cases\n",
    "* Artificial Neural Networks for Regression and Classification\n",
    "* Convolutional Neural Networks for Computer Vision\n",
    "* Recurrent Neural Networks for Time Series Analysis\n",
    "* Self Organizing Maps for Feature Extraction\n",
    "* Deep Boltzmann Machines for Recommendation Systems\n",
    "* Auto Encoders for Recommendation Systems\n",
    "\n",
    "#### Artificial Neural Network\n",
    "1. Randomly initialise the weights to small numbers close to 0(but not 0).\n",
    "2. Input the first observation of your dataset in the input layer, each feature is one input node.\n",
    "3. Forward propagation - from left to right, the neurons are activated in a way that the impact of each neuron's activation is limited by the weights. Propagate the activations until getting the predicted result y.\n",
    "4. Compare the predicted result to the actual result. Measure the generated error.\n",
    "5. Back-Propagation- from right to left, the error is back propagated. Update the weights according to how much they are responsible for the error. The learning rate decides by how much we update the weights.\n",
    "6. Repeat Step 1 to 5 and update the weights after each observation(Reinforcement Learning). Or Repeat Step 1 to 5 but update the weights only after a batch of obserbations(Batch Learning).\n",
    "7. When the whole training set passed through the ANN, that makes an epoch. Repeat more epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "An activation function is a function used to compute the output in a layer of a Neural Network. Main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.\n",
    "\n",
    "It’s just a thing (node) that you add to the output end of any neural network. It is also known as Transfer Function. It can also be attached in between two Neural Networks.\n",
    "\n",
    "In simple words artificial neuron calculates a \"weighted sum\" of its input, adds a bias and then decides whether it should be “fired” or not ( yeah right, an activation function does this ).\n",
    "\n",
    "**Y = SUM(weight * input) + bias**\n",
    "\n",
    "Now, the value of Y can be anything ranging from -inf to +inf. The neuron really doesn’t know the bounds of the value. So we decided to add “activation functions” for this purpose. To check the Y value produced by a neuron and decide whether outside connections should consider this neuron as “fired” or not. Or rather let’s say — “activated” or not.\n",
    "\n",
    "**Most popular Activation Functions:**\n",
    "![Activation Functions](images/activation-function.png)\n",
    "![Activation Functions](images/activation-functions-cheatsheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Rectifier function(“relu”) for hidden layer. Sigmoid function is for output layer where probability is expected. Binary outcome Sigmoid activation function, if category outcome then SoftMax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: How do we use sigmoid? When do we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we use sigmoid\n",
    "Sigmoid activation function is used for output layer in in Neural Networks where probability is expected.\n",
    "\n",
    "**Code snippet:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Sigmoid in ANN!\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n",
    "\n",
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use:\n",
    "Sigmoid function is used in neural networks to give logistic neurons real-valued output that is a smooth and bounded function of their total input. The sigmoid function is a activation function in terms of underlying gate structured in co-relation to Neurons firing, in Neural Networks. \n",
    "\n",
    "Benifit:\n",
    "- Sigmoid function is nonlinear even in combination. It can stack layers. As it has smooth gradient so can work for non binary activations and produce analog activation.\n",
    "![SigmoidFunction](images/sigmoid-func.png)\n",
    "- As we see value of Y is very steep for value of X in between -2 to 2. Which mean small change in value of X can cause significent change in the value of Y. It means function has tendency to fall in either side of curve for Y value. That is good for classification by making clear distinctions on prediction.\n",
    "- Anathor advantage is, Sigmoid activation function always results in bound range binary outcome (0,1) in place of (-inf, inf) of linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What do we use to measure our models? Describe four parameters / metrics with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of machine learning model is an essential part of project. Mostly we use classification accurecy to measure the performance of the model. Some thime it is not sufficient to judge performance. Lets look some other evolution matrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classification Accuracy \n",
    "Accurecy usualy masured using Classification Accuracy. It is ratio of correct predictions devided by total number of predictions. It works quite well when data set has equal number of samples from each class. \n",
    "![Accuracy](images/accuracy.gif)\n",
    "The best value is 1 and the worst value is 0.\n",
    "\n",
    "### 2. Loss Functions\n",
    "Cost(loss) Function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y.( estimate how badly models are performing)\n",
    "Models learn by minimizing a cost function, you may naturally wonder how the cost function is minimized — enter gradient descent. Gradient descent is an efficient optimization algorithm that attempts to find a local or global minima of a function.\n",
    "\n",
    "Logarithmic Loss or **Log Loss**, works by penalising the false classifications. It works well for multi-class classification. **Cross-entropy loss**, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
    "![LossFunctions](images/LossFunctions1.png)\n",
    "Higher the value, The worse off the prediction is from target.\n",
    "\n",
    "### 3. Confusion Matrix\n",
    "Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model.\n",
    "There are 4 important terms :\n",
    "\n",
    "* True Positives : The cases in which we predicted YES and the actual output was also YES.\n",
    "* True Negatives : The cases in which we predicted NO and the actual output was NO.\n",
    "* False Positives : The cases in which we predicted YES and the actual output was NO.\n",
    "* False Negatives : The cases in which we predicted NO and the actual output was YES.\n",
    "\n",
    "Accuracy for the matrix can be calculated as:\n",
    "\n",
    "Accuracy = TP+TN / (TP+TN+FP+FN)\n",
    "\n",
    "![ConfusionMatrix](images/ConfusionMatrix.png)\n",
    "\n",
    "### 4. Classification Report : F1 Score\n",
    "F1 Score is used to measure a test’s accuracy. F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is (0, 1). It tells, how precise classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).\n",
    "![f1 Score](images/f1-score.gif)\n",
    "The greater the F1 Score, the better is the performance of our model. F1 score reaches its best value at 1 and worst score at 0.\n",
    "\n",
    "**precision**\n",
    "Measures the fraction(ratio) of actual positives among those examples that are predicted as positive. \n",
    "The best value is 1 and the worst value is 0.\n",
    "Precision = tp / (tp + fp)\n",
    "\n",
    "**recall**\n",
    "Measures the fraction of actual positives that are predicted as positive. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "The best value is 1 and the worst value is 0.\n",
    "Recall = tp / (tp + fn)\n",
    "\n",
    "### 5. R squared\n",
    "In statistics, the coefficient of determination, denoted R² or r² and pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variable. (Fraction of variance in dependent variable which is explained by independent variable). \n",
    "R2 = 0 bad model. No linear relationship\n",
    "R2 = 1 good model. Line is perfectly fits the data.\n",
    "\n",
    "R2 = Explained variation / Total of Variation\n",
    "\n",
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15624510</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15810944</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15668575</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15603246</td>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15804002</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User ID  Gender  Age  EstimatedSalary  Purchased\n",
       "0  15624510    Male   19            19000          0\n",
       "1  15810944    Male   35            20000          0\n",
       "2  15668575  Female   26            43000          0\n",
       "3  15603246  Female   27            57000          0\n",
       "4  15804002    Male   19            76000          0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "df = pd.read_csv('dataset/Social_Network_Ads.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, [2, 3]].values\n",
    "y = df.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mk194903/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/mk194903/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/mk194903/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state = 100)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mk194903/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Let's \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression(random_state = 0)\n",
    "logmodel.fit(X_train, y_train)\n",
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  85.0\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score \n",
    "print (\"Accuracy : \", accuracy_score(y_test,predictions)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is percentage higher the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62  3]\n",
      " [12 23]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative diagonal should be 0 for best result. It means 12 negative(0) results went wrong and 3 positive(1) results did not predicted correct. Accurecy = (62+23)/(62+23+3+12)=> 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89        65\n",
      "           1       0.88      0.66      0.75        35\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       100\n",
      "   macro avg       0.86      0.81      0.82       100\n",
      "weighted avg       0.85      0.85      0.84       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For best results F-Score should be close to 1. In above case for prediction of 1 is not that great as prediction of 0. Which is same we saw in confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE L1 loss - Should be close to 0\n",
    "from sklearn.metrics import mean_absolute_error  \n",
    "mean_absolute_error(y_test,predictions) #y_target, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 Loss value is closer to 0. Which is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE L2 loss - Should be close to 0\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mean_squared_error(y_test,predictions) #y_target, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 Loss value is closer to 0. Which is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.1808404471595075"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log loss function\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Loss has no upper bound and it exists on the range (0, ∞). Log Loss closer to 0 indicates higher accuracy, whereas if the Log Loss is away from 0 then it indicates lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34065934065934056"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R Squared \n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, predictions)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 is applicable to linear model. In this case not of much use. For linear model value should be near to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [A Survey of Text Similarity Approaches](https://pdfs.semanticscholar.org/5b5c/a878c534aee3882a038ef9e82f46e102131b.pdf)\n",
    "* [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)\n",
    "* [Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)\n",
    "* [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)\n",
    "* [Interns Explain Basic Neural Network](https://blog.datawow.io/interns-explain-basic-neural-network-ebc555708c9)\n",
    "* [Metrics to Evaluate your Machine Learning Algorithm](https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234)\n",
    "* [A practical explanation of a Naive Bayes classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)\n",
    "* [A Deep Dive Into Sklearn Pipelines](https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines)\n",
    "* [Pre-Processing in Natural Language Machine Learning](https://towardsdatascience.com/pre-processing-in-natural-language-machine-learning-898a84b8bd47)\n",
    "* [SVM](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)\n",
    "* [An Intuitive Understanding of Word Embeddings](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
